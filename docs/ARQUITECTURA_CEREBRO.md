#  Anatom铆a del Cerebro TidalAI

Este documento detalla la arquitectura t茅cnica, el funcionamiento interno y las fuentes de datos del sistema de Inteligencia Artificial de TidalAI Companion.

---

## 1. Arquitectura del Sistema

El "cerebro" no es una red neuronal masiva (como GPT), sino un modelo probabil铆stico ligero y eficiente dise帽ado para correr en una Raspberry Pi con latencia cero.

```mermaid
graph TD
    A[Fuentes de Datos] -->|Script de Extracci贸n| B(patterns.txt)
    B -->|Tokenizaci贸n| C[Markov Model Class]
    C -->|Entrenamiento| D{Tabla de Probabilidades}
    E[Usuario Web] -->|Request + Temperatura| F[Generador]
    D --> F
    F -->|Patr贸n Nuevo| G[Salida TidalCycles]
    G -->|Feedback| H[Favorites.json]
    H -->|Re-entrenamiento| B
```

## 2. El Corpus (La Memoria)

El corpus es la base de conocimiento del sistema. Actualmente contiene **~440 patrones 煤nicos** derivados de tres fuentes:

| Fuente | Descripci贸n | Aporte al Conocimiento |
|---|---|---|
| **Samples Locales** | Base original (`bd`, `sn`, `hh`) | Estructuras r铆tmicas b谩sicas y nombres de samples reales. |
| **G茅neros Musicales** | `06_generos.html` | Estructuras de Techno, House, DnB, Trap, etc. |
| **Documentaci贸n T茅cnica** | `04_efectos`, `07_tecnicas` | Uso correcto de sintaxis, efectos (`# lpf`), y funciones complejas (`every`, `jux`). |

### Estructura de los Datos
El sistema almacena "frases musicales completas". No aprende notas sueltas, aprende **contextos**.
*   **Input**: `d1 $ every 4 (fast 2) $ s "bd sn"`
*   **Conceptos aprendidos**:
    *   Que `every` suele ir seguido de un n煤mero (`4`).
    *   Que `fast` es una transformaci贸n v谩lida dentro de `every`.
    *   Que `s` define el sonido final.

## 3. El Motor: Cadenas de Markov (Markov Chains)

El n煤cleo algor铆tmico es una **Cadena de Markov de Orden Variable** (t铆picamente Orden 2 o 3).

### 3.1 Tokenizaci贸n Inteligente
El sistema no lee letra por letra. Usa un tokenizador l茅xico espec铆fico para TidalCycles:
*   Frase: `sound "bd(3,8)"`
*   Tokens: `['sound', '"bd', '(', '3', ',', '8', ')', '"']`

Esto evita errores de sintaxis (ej. nunca romper谩 la palabra `sound` en `so` + `und`).

### 3.2 Probabilidad de Transici贸n (N-Grams)
El cerebro construye un mapa de probabilidades basado en el historial reciente (N-Grams).

**Ejemplo Simplificado:**
Si el modelo ha escrito: `sound "bd`

Mira en su tabla de memoria: *"驴Qu茅 suele venir despu茅s de `sound "bd`?"*

*   Opci贸n A: `*4"` (Probabilidad 40% - *Techno*)
*   Opci贸n B: `(3,8)"` (Probabilidad 30% - *Euclidiano*)
*   Opci贸n C: `~ sn"` (Probabilidad 20% - *Breakbeat*)
*   Opci贸n D: `[sn cp]"` (Probabilidad 10% - *Complejo*)

### 3.3 Temperatura (Creatividad)
El par谩metro "Temperatura" (`temp`) altera estas probabilidades matem谩ticamente antes de tirar el dado:

*   **Temp 0.5 (Fr铆o)**: Exagera las probabilidades altas. Casi siempre elegir谩 la Opci贸n A. Resultado: M煤sicalmente seguro, repetitivo.
*   **Temp 1.5 (Caliente)**: Iguala las probabilidades. La Opci贸n D (rara) tiene casi tantas chances como la A. Resultado: Ca贸tico, innovador, sorpresivo.

## 4. Ciclo de Vida del Dato

1.  **Ingesta**: Python scripts (`import_all_docs.py`) leen archivos HTML, limpian la basura y extraen solo el c贸digo.
2.  **Entrenamiento**: Al arrancar, el servidor Flask lee `patterns.txt` y construye las matrices de transici贸n en RAM (< 50ms).
3.  **Inferencia**: Cuando pulsas "Generar", el modelo camina por la cadena de Markov hasta generar un patr贸n v谩lido.
4.  **Evoluci贸n**: Si guardas un patr贸n en Favoritos, este se a帽ade al corpus, haciendo que esas transiciones sean m谩s probables en el futuro.

## 5. Capacidades Actuales

Gracias a la 煤ltima inyecci贸n de datos, tu cerebro artificial domina:
*   **Polimetr铆a**: `{3, 4}`
*   **Funciones Estoc谩sticas**: `degrade`, `sometimes`
*   **S铆ntesis**: `superpiano`, `supersaw`
*   **Efectos**: Toda la cadena de `lpf`, `room`, `shape`, `crush`.

## 6. Inteligencia Colectiva y Evoluci贸n (Pesos Est茅ticos)

A partir de la v1.5, el Cerebro ha dejado de ser un modelo est谩tico para convertirse en uno **configurable y evolutivo**.

### 6.1 Pesos de Gusto Art铆stico
El usuario puede alterar la "personalidad" de la IA mediante pesos din谩micos en `config_evolution.json`:
*   **Densidad**: Prioriza patrones con m谩s eventos por ciclo.
*   **Variedad**: Fomenta el uso de tokens 煤nicos (evita la monoton铆a).
*   **Complejidad**: Premia el uso de funciones avanzadas (`jux`, `every`, `struct`).
*   **Groove (Euclidiano)**: Valora positivamente las estructuras matem谩ticas de tipo `(k, n)`.

### 6.2 El Algoritmo Gen茅tico (Ronda Nocturna)
El sistema ejecuta procesos de **Evoluci贸n por Selecci贸n**:
1.  **Mutaci贸n**: Genera 50-100 candidatos con alta temperatura.
2.  **Evaluaci贸n**: Aplica los pesos est茅ticos para dar una puntuaci贸n a cada uno.
3.  **Supervivencia**: Los 10 mejores patrones son inyectados de vuelta al `patterns.txt`.
4.  **Auto-aprendizaje**: El modelo se re-entrena autom谩ticamente con la nueva "sabidur铆a" colectiva.

## 7. Introspecci贸n y Visualizaci贸n

Para evitar el efecto de "caja negra", hemos abierto las puertas al interior del Cerebro.

### 7.1 Mapa Mental (D3 Graph)
Una representaci贸n topol贸gica de los **Tokens**. Permite ver los "cl煤sters de pensamiento":
*   Los nodos son palabras del lenguaje Tidal.
*   Los enlaces son las transiciones probables aprendidas.
*   El tama帽o del nodo indica la frecuencia de uso (importancia).

### 7.2 Flujo de Pensamiento (Live Thought Stream)
Durante la generaci贸n, el sistema exporta su **mon贸logo interno**:
*   Muestra el token elegido actualmente.
*   Muestra las **3 alternativas descartadas** con sus porcentajes de probabilidad.
*   Permite al usuario entender *por qu茅* la IA eligi贸 un sample de bombo o una funci贸n de filtro.

---
## 8. Procesamiento Sem谩ntico (El Or谩culo)

El Or谩culo act煤a como una capa de traducci贸n entre el lenguaje natural y los par谩metros t茅cnicos.

*   **Motor**: `oracle_engine.py` usa un modelo de scoring basado en l茅xicos ponderados.
*   **Mapeo**:
    *   *Intenci贸n*: "m谩s tribal y oscuro"
    *   *Traducci贸n*: `{density_offset: +0.2, style_pref: 'organic', extra_tokens: ['# lpf 1000']}`
*   **Pipeline**: El resultado del Or谩culo se inyecta como modificador en el generador de patrones antes de la inferencia.

## 9. Orquestaci贸n Polif贸nica (Poly-Spread)

Para evitar la limitaci贸n de sobreescritura de par谩metros en TidalCycles, el sistema implementa un **Layer Splitter**.

1.  **Parseo**: `get_layers()` rompe el string original usando `#` como delimitador.
2.  **Identificaci贸n de Fuentes**: Separa los bloques que definen sonido (`s`, `sound`) de los efectos.
3.  **Distribuci贸n**: Si se encuentran `N` fuentes de sonido, se generan `N` mensajes OSC para canales contiguos (`d1`, `d2`, ...).
4.  **Herencia**: Los efectos comunes (ej: `# room 0.5`) se replican en todas las capas para mantener la integridad del dise帽o sonoro.

---

## 10. The Intelligent Theorist: Validaci贸n Musical (Phase 17)

Introducido en v4.3, el `TheoryEngine` act煤a como un **juez musical** que valida patrones contra reglas de teor铆a.

### 10.1 Arquitectura
```python
class TheoryEngine:
    def __init__(self):
        self.rules = self._load_rules('theory_rules.json')
    
    def validate(self, pattern, genre):
        for rule in self.rules[genre]:
            if rule['active'] and not self._check_rule(pattern, rule):
                return False, rule['message']
        return True, None
```

### 10.2 Tipos de Reglas
- **Hardcoded**: Funciones Python que analizan la estructura del patr贸n (e.g., `_rule_kick_on_one`).
- **Regex**: Expresiones regulares definidas en JSON para detecci贸n de patrones (e.g., `"bd.*~"` para bombos sincopados).

### 10.3 Bucle de Validaci贸n
En `app.py`, el endpoint `/api/generate` implementa un **retry loop**:
1. Genera patr贸n con IA.
2. Valida contra `TheoryEngine`.
3. Si falla, reintenta hasta 3 veces con `temperature += 0.1`.
4. Retorna `validation_info` en JSON con el resultado.

### 10.4 Rules Editor (Phase 17b)
Interfaz web para gesti贸n din谩mica de reglas:
- **Toggle ON/OFF**: Activar/desactivar reglas por g茅nero.
- **Custom Regex**: A帽adir reglas personalizadas basadas en expresiones regulares.
- **Persistencia**: `theory_rules.json` se actualiza en tiempo real.

---

## 11. Latent Space Navigation: Interpolaci贸n Vectorial (Phase 18)

Introducido en v4.4, el `LatentEngine` permite mezclar g茅neros matem谩ticamente.

### 11.1 Arquitectura
```python
class LatentEngine:
    def __init__(self):
        self.genre_vectors = {
            "techno": {"density": 0.8, "complexity": 0.6, "tempo": 140},
            "ambient": {"density": 0.3, "complexity": 0.4, "tempo": 90}
        }
    
    def blend_multiple(self, blend_config):
        # blend_config = {"techno": 0.7, "ambient": 0.3}
        result = {}
        for genre, weight in blend_config.items():
            vec = self.genre_vectors[genre]
            for param, value in vec.items():
                result[param] = result.get(param, 0) + (value * weight)
        return result
```

### 11.2 Interpolaci贸n Lineal
Cada g茅nero es un "vector" en un espacio de par谩metros. La mezcla se calcula como:
```
resultado = (vector_A * peso_A) + (vector_B * peso_B)
```

**Ejemplo**:
```
70% Techno + 30% Ambient:
density = (0.8 * 0.7) + (0.3 * 0.3) = 0.65
tempo = (140 * 0.7) + (90 * 0.3) = 125
```

### 11.3 Single Source of Truth
Los g茅neros disponibles se leen desde `theory_rules.json`, manteniendo consistencia entre `TheoryEngine` y `LatentEngine`.

### 11.4 Validaci贸n Ponderada
En modo blend, las reglas del Theorist se aplican seg煤n el peso:
- **G茅nero dominante (>50%)**: Reglas obligatorias.
- **G茅nero secundario (<50%)**: Reglas opcionales (advertencias).

---
*Documento actualizado v4.4 - Enero 2026*
*TidalAI Assistant Core*
